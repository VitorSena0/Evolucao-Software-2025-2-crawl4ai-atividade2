{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# C√âLULA 0 ‚Äî CHECK DE GPU\n",
        "# ===============================\n",
        "\n",
        "import torch, sys\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA dispon√≠vel:\", torch.cuda.is_available())\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\n",
        "        \"‚ùå GPU N√ÉO ATIVA\\n\"\n",
        "        \"Ative em: Ambiente de execu√ß√£o ‚Üí Alterar tipo ‚Üí GPU\\n\"\n",
        "        \"Depois REINICIE o runtime.\"\n",
        "    )\n",
        "\n",
        "print(\"GPU detectada:\", torch.cuda.get_device_name(0))\n",
        "print(\"‚úÖ GPU OK ‚Äî pode continuar\")\n"
      ],
      "metadata": {
        "id": "EYPZ2Lx3Xx4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# C√âLULA 1 ‚Äî HUGGING FACE LOGIN\n",
        "# ===============================\n",
        "\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "if \"HUGGINGFACE_TOKEN\" in os.environ:\n",
        "    login(token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
        "    print(\"‚úÖ Hugging Face autenticado com token\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è HUGGINGFACE_TOKEN n√£o encontrado\")\n",
        "    print(\"‚û°Ô∏è Acesso p√∫blico ser√° usado (ok para Qwen 7B)\")\n"
      ],
      "metadata": {
        "id": "fu4pb6wEY1Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# C√âLULA 2 ‚Äî SETUP\n",
        "# ===============================\n",
        "\n",
        "import os, sys, subprocess\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "def pip_install(args):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\"] + args.split())\n",
        "\n",
        "def pip_uninstall(pkg):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# N√ÉO reinstala torch (Colab j√° vem com CUDA correto)\n",
        "pip_install(\"install -q -U transformers accelerate huggingface_hub\")\n",
        "\n",
        "pip_uninstall(\"bitsandbytes\")\n",
        "pip_install(\"install -q -U bitsandbytes\")\n",
        "\n",
        "print(\"‚úÖ Ambiente configurado\")\n"
      ],
      "metadata": {
        "id": "67xTrJ4kXtTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# C√âLULA 3 ‚Äî MODELO QWEN 2.5 7B\n",
        "# ===============================\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# Tokenizer (leve, n√£o ocupa VRAM)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Quantiza√ß√£o 4-bit otimizada para T4\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# üö´ NUNCA usar device_map=\"auto\" no Colab Free\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},          # for√ßa TUDO na GPU\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,      # evita estouro de RAM\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ Qwen2.5-7B carregado com sucesso\")\n",
        "print(\"VRAM usada (GB):\", round(torch.cuda.memory_allocated() / 1024**3, 2))\n"
      ],
      "metadata": {
        "id": "5M4Yd7j9X0LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# C√âLULA 4 ‚Äî INFER√äNCIA SEGURA\n",
        "# ===============================\n",
        "\n",
        "def ask_qwen(\n",
        "    prompt,\n",
        "    max_new_tokens=700,\n",
        "    temperature=0.2,\n",
        "    top_p=0.95,\n",
        "    max_prompt_tokens=6000,  # CONTROLE REAL\n",
        "    show_usage=False\n",
        "):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Voc√™ √© um especialista em Engenharia de Software e DevOps.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        truncation=True,\n",
        "        max_length=max_prompt_tokens,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    if show_usage:\n",
        "        print(\"Prompt tokens:\", input_ids.shape[-1])\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return decoded.split(messages[-1][\"content\"])[-1].strip()\n"
      ],
      "metadata": {
        "id": "Qel9MHR7X3dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# C√âLULA 5 ‚Äî TESTE\n",
        "# ===============================\n",
        "\n",
        "resp = ask_qwen(\n",
        "    \"Explique em 3 linhas o que √© GitFlow.\",\n",
        "    show_usage=True\n",
        ")\n",
        "\n",
        "print(resp)\n"
      ],
      "metadata": {
        "id": "HAUP-c-xX5Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# C√âLULA 6 ‚Äî COLETA GIT\n",
        "# ===============================\n",
        "\n",
        "import subprocess, os\n",
        "from collections import Counter\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def git(cmd, cwd):\n",
        "    return subprocess.check_output(cmd, cwd=cwd, shell=True).decode().strip()\n",
        "\n",
        "repo_url = \"https://github.com/unclecode/crawl4ai.git\"\n",
        "repo_dir = \"/content/repo\"\n",
        "\n",
        "if not os.path.exists(repo_dir):\n",
        "    subprocess.check_call([\"git\", \"clone\", repo_url, repo_dir])\n",
        "\n",
        "branches = git(\"git branch -r | grep -v HEAD\", repo_dir).splitlines()\n",
        "branches = [b.replace(\"origin/\", \"\").strip() for b in branches]\n",
        "\n",
        "tags = git(\"git tag --list\", repo_dir).splitlines()\n",
        "\n",
        "bucket = Counter(b.split('/')[0] if '/' in b else 'root' for b in branches)\n",
        "\n",
        "display(Markdown(f\"- **Branches:** {len(branches)}\"))\n",
        "display(Markdown(f\"- **Tags:** {len(tags)}\"))\n",
        "display(Markdown(f\"```{bucket}```\"))\n"
      ],
      "metadata": {
        "id": "GKJgQTEwX67e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# C√âLULA 7 ‚Äî AN√ÅLISE\n",
        "# ===============================\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Analise o modelo de branching com base nos dados reais.\n",
        "\n",
        "Branches totais: {len(branches)}\n",
        "Distribui√ß√£o:\n",
        "{bucket}\n",
        "\n",
        "Amostra:\n",
        "{sorted(branches)[:40]}\n",
        "\n",
        "Classifique:\n",
        "A) GitFlow\n",
        "B) GitHub Flow\n",
        "C) Trunk-Based Development\n",
        "\n",
        "Justifique com evid√™ncias reais.\n",
        "\"\"\"\n",
        "\n",
        "resp = ask_qwen(prompt)\n",
        "display(Markdown(\"## üåø Branching Model\"))\n",
        "display(Markdown(resp))\n"
      ],
      "metadata": {
        "id": "u6Cv4SjhX8O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# C√âLULA 8 ‚Äî RELEASES\n",
        "# ===============================\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Analise a estrat√©gia de releases com base nas tags abaixo.\n",
        "\n",
        "Total de tags: {len(tags)}\n",
        "Amostra de tags:\n",
        "{tags[-20:]}\n",
        "\n",
        "Classifique:\n",
        "- Rapid Release\n",
        "- Release Train\n",
        "- LTS + Current\n",
        "- Ad-hoc\n",
        "\"\"\"\n",
        "\n",
        "print(ask_qwen(prompt))\n"
      ],
      "metadata": {
        "id": "ze-8ftsdWpqF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}