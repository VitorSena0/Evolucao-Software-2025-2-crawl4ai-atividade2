{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-agUnQ1lqLbU"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/unclecode/crawl4ai.git"
      ],
      "metadata": {
        "id": "2xLYQQBpt_Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entra na pasta do repositório\n",
        "%cd /content/crawl4ai\n",
        "\n",
        "# Verifica se agora você está no lugar certo (deve mostrar o caminho terminando em /crawl4ai)\n",
        "!pwd"
      ],
      "metadata": {
        "id": "DIRuQgIUJYSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salva o histórico de merges das branches\n",
        "!git log --graph --oneline --all --decorate --merges > /content/historico_merges.txt"
      ],
      "metadata": {
        "id": "AApWBYwNGmGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "Wi02UGrZtcvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lê os principais arquivos com informações relacionadas às releases e as branches\n",
        "changelog = \"\"\n",
        "releases = \"\"\n",
        "merges = \"\"\n",
        "with open(\"/content/crawl4ai/CHANGELOG.md\", \"r\") as f:\n",
        "  changelog = f.read()\n",
        "with open(\"/content/crawl4ai/.github/workflows/release.yml\", \"r\") as f:\n",
        "  releases = f.read()\n",
        "with open(\"/content/historico_merges.txt\", \"r\") as f:\n",
        "  merges = f.read()"
      ],
      "metadata": {
        "id": "2cFXipfaRqYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# criação do prompt\n",
        "system_msg = f\"\"\"Você é um Especialista em Engenharia de Software com foco em Gerenciamento de Configuração. Sua tarefa é analisar o repositório \"Crawl4AI\" para identificar sua Estratégia de Release e seu Fluxo de Trabalho (Branching Model).\n",
        "\n",
        "PRINCIPAIS ESTRATÉGIAS DE WORKFLOW PARA PROCURAR:\n",
        "- Gitflow: Presença de branches persistentes 'main' E 'develop'. Merges de features vão para 'develop'. Existem branches de 'release/v*'.\n",
        "- GitHub Flow: Apenas a branch 'main' é persistente. Features branches são curtas e mergeadas direto na 'main'.\n",
        "- Trunk-based: Commits frequentes e diretos na 'main' (ou via PRs curtíssimos). O histórico é quase uma linha única e reta.\n",
        "\n",
        "PRINCIPAIS ESTRATÉGIAS DE RELEASE PARA PROCURAR:\n",
        "- Rapid Release: Alta frequência de versões (tags v* constantes) baseadas em novas funcionalidades prontas.\n",
        "- Release Train: Versões em datas fixas e previsíveis.\n",
        "- LTS + Current: Coexistência de versões estáveis de longo suporte com versões de ponta.\n",
        "\n",
        "VOCÊ RECEBERÁ DADOS DOS SEGUINTES ARQUIVOS:\n",
        "- CHANGELOG.md: Registro histórico de versões e datas.\n",
        "- release.yml: Configuração de automação do GitHub Actions.\n",
        "- Histórico de Merges: O log visual (graph) da árvore de commits.\n",
        "\n",
        "METODOLOGIA DE ANÁLISE:\n",
        "1. Examine o 'release.yml': Tente identificar o que dispara o deploy.\n",
        "2. Examine o 'CHANGELOG.md': Tente identificar o intervalo médio entre versões.\n",
        "3. Examine o 'Histórico de Merges': Procure pela existência da branch 'develop' e branches com prefixo 'release/v*', 'feature/*' ou 'fix/* e verificar a persistência dessas branches'.\n",
        "\n",
        "CONCLUSÃO:\n",
        "Apresente sua conclusão baseada na predominância das evidências. Se houver elementos de mais de um modelo (ex: um fluxo Gitflow simplificado com ritmo de Rapid Release), descreva essa governança híbrida.\"\"\"\n",
        "user_msg = f\"\"\"\n",
        "Aqui estão os arquivos a serem analisados:\n",
        "CHANGELOG.md:\n",
        "{\"\\n\".join(changelog.splitlines()[-262:])}\n",
        "\n",
        "release.yml:\n",
        "{releases}\n",
        "\n",
        "Histórico de Merges:\n",
        "{merges}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SDUO5TyVThmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modelo\n",
        "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "\n",
        "# Configuração de Quantização usando bitsandbytes\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Carregar Tokenizer e Modelo\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=False,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"sdpa\"\n",
        ")"
      ],
      "metadata": {
        "id": "ATBn6B1St2SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "        {\"role\": \"system\", \"content\": system_msg},\n",
        "        {\"role\": \"user\", \"content\": user_msg},\n",
        "    ]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "  messages,\n",
        "  add_generation_prompt=True,\n",
        "  tokenize=True,\n",
        "  return_dict=True,\n",
        "  return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "#. Gerar a Resposta\n",
        "outputs = model.generate(**inputs, max_new_tokens=1500, use_cache=True, do_sample=True, temperature=0.2, top_p=0.9, repetition_penalty=1.1)\n",
        "with open(\"/content/resposta.md\", \"w\") as f:\n",
        "  f.write(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "OQaItAE5u0h4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}