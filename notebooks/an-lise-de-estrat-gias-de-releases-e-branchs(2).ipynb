{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## CÃ‰LULA 1 (corrigida): CodeLlama no Kaggle (T4x2) com controle de contexto (16k) e estabilidade\n\nimport os, sys, subprocess\nimport torch\n\n# Estabilidade: use sÃ³ 1 GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\ndef pip_install(args: str):\n    print(f\"Rodando: python -m pip {args}\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\"] + args.split())\n\ndef pip_uninstall(pkg: str):\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg])\n    except Exception:\n        pass\n\n# Instala libs base\npip_install(\"install -q --no-cache-dir -U transformers accelerate\")\n\n# bitsandbytes compatÃ­vel (recente)\npip_uninstall(\"bitsandbytes\")\npip_install(\"install -q --no-cache-dir -U bitsandbytes\")\n\nimport transformers, accelerate\nprint(\"torch:\", torch.__version__)\nprint(\"transformers:\", transformers.__version__)\nprint(\"accelerate:\", accelerate.__version__)\n\nbnb_ok = True\ntry:\n    import bitsandbytes as bnb\n    print(\"bitsandbytes:\", bnb.__version__)\nexcept Exception as e:\n    bnb_ok = False\n    print(\"bitsandbytes import falhou, vou usar fallback fp16. Erro:\", repr(e))\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nlocal_model = \"/kaggle/input/codellama/hf/7b-instruct\"\nmodel_id = local_model if os.path.exists(local_model) else \"codellama/CodeLlama-7b-Instruct-hf\"\nprint(f\"Carregando modelo de: {model_id}\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n\nif bnb_ok:\n    from transformers import BitsAndBytesConfig\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        quantization_config=bnb_config,\n        device_map={\"\": 0},\n    )\n    precision_mode = \"4-bit (bitsandbytes)\"\nelse:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        torch_dtype=torch.float16,\n        device_map={\"\": 0},\n    )\n    precision_mode = \"fp16 (fallback)\"\n\nMAX_CTX = int(getattr(model.config, \"max_position_embeddings\", 4096))\nprint(\"max_position_embeddings:\", MAX_CTX)\n\ndef ask_codellama(\n    prompt_text: str,\n    max_new_tokens: int = 512,\n    temperature: float = 0.2,\n    top_p: float = 0.95,\n    show_token_usage: bool = False,\n):\n    \"\"\"\n    - Trunca o prompt para caber no contexto do modelo\n    - Ajusta max_new_tokens para nÃ£o ultrapassar MAX_CTX (prompt + saÃ­da)\n    \"\"\"\n    prompt_template = f\"<s>[INST] {prompt_text} [/INST]\"\n    device = next(model.parameters()).device\n\n    inputs = tokenizer(\n        prompt_template,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=MAX_CTX,\n    ).to(device)\n\n    prompt_len = inputs[\"input_ids\"].shape[-1]\n    allowed_new = max(0, MAX_CTX - prompt_len)\n    max_new = min(max_new_tokens, allowed_new)\n\n    if show_token_usage:\n        print(f\"prompt_tokens={prompt_len}/{MAX_CTX} | allowed_new={allowed_new} | using_max_new={max_new}\")\n\n    with torch.inference_mode():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=max_new,\n            do_sample=True,\n            temperature=temperature,\n            top_p=top_p,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n    text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return text.split(\"[/INST]\")[-1].strip()\n\nprint(f\"\\nModelo pronto. Modo: {precision_mode}\")\nprint(\"torch.cuda.device_count():\", torch.cuda.device_count())\nif torch.cuda.is_available():\n    print(\"GPU 0:\", torch.cuda.get_device_name(0))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T23:30:39.355469Z","iopub.execute_input":"2025-12-16T23:30:39.355980Z","iopub.status.idle":"2025-12-16T23:33:42.198100Z","shell.execute_reply.started":"2025-12-16T23:30:39.355939Z","shell.execute_reply":"2025-12-16T23:33:42.197318Z"}},"outputs":[{"name":"stdout","text":"Rodando: python -m pip install -q --no-cache-dir -U transformers accelerate\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.0/44.0 kB 3.6 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12.0/12.0 MB 205.9 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 380.9/380.9 kB 399.0 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 MB 296.9 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 363.4/363.4 MB 260.7 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13.8/13.8 MB 252.6 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 24.6/24.6 MB 266.0 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 883.7/883.7 kB 246.3 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 664.8/664.8 MB 219.5 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 211.5/211.5 MB 88.7 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 56.3/56.3 MB 88.6 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 127.9/127.9 MB 100.5 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 207.5/207.5 MB 112.4 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 21.1/21.1 MB 110.0 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\nWARNING: Skipping bitsandbytes as it is not installed.\n","output_type":"stream"},{"name":"stdout","text":"Rodando: python -m pip install -q --no-cache-dir -U bitsandbytes\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 59.1/59.1 MB 233.0 MB/s eta 0:00:00\ntorch: 2.6.0+cu124\ntransformers: 4.57.3\naccelerate: 1.12.0\nbitsandbytes: 0.49.0\nCarregando modelo de: codellama/CodeLlama-7b-Instruct-hf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57f3b4e6cec64ceb8e32b5b3bd98797b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07cce454dbbb46db99e09b71e137b7cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee2f99e375a94d6da61ae8305c3c72ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47976aa8957b4887a363d032f703339c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/646 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0773e40b69f14c219f20804e147968c0"}},"metadata":{}},{"name":"stderr","text":"2025-12-16 23:32:21.025738: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765927941.249360      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765927941.322009      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9683d34ea05745a1b42890972f05493a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cac0df985d74b449dc39c510dfacd5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34196c052e224a1288038a5ec72d9a66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb55cb3826d843d6ae12c63fc26a8821"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f180169a23e44c4fa7bc0099b3dadec5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f05df76795c429795ffff11660fbda1"}},"metadata":{}},{"name":"stdout","text":"max_position_embeddings: 16384\n\nModelo pronto. Modo: 4-bit (bitsandbytes)\ntorch.cuda.device_count(): 1\nGPU 0: Tesla T4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from IPython.display import display, Markdown\nimport subprocess, os, glob, re\nfrom collections import Counter\nfrom datetime import datetime\n\ndef run_git_command(command, cwd):\n    try:\n        result = subprocess.check_output(\n            command, cwd=cwd, shell=True, stderr=subprocess.STDOUT\n        ).decode(\"utf-8\")\n        return result.strip()\n    except subprocess.CalledProcessError as e:\n        return f\"ERRO AO EXECUTAR GIT: {e.output.decode('utf-8')}\"\n\nrepo_url = \"https://github.com/unclecode/crawl4ai.git\"\nrepo_dir = \"/kaggle/working/crawl4ai_activity2\"\n\nif os.path.exists(repo_dir):\n    print(f\"RepositÃ³rio jÃ¡ existe em {repo_dir}. Reutilizando...\")\nelse:\n    print(f\"Clonando {repo_url} em {repo_dir} ...\")\n    subprocess.check_call(f\"git clone {repo_url} {repo_dir}\".split())\n\nos.chdir(repo_dir)\nprint(f\"DiretÃ³rio de trabalho: {os.getcwd()}\")\n\n# --- Branches ---\nbranches_all = run_git_command(\"git branch -r | grep -v 'HEAD' | sed 's/^ *//'\", repo_dir).splitlines()\nbranches_all = [b.replace(\"origin/\", \"\", 1) if b.startswith(\"origin/\") else b for b in branches_all]\nbranches_all = [b.strip() for b in branches_all if b.strip()]\nbranches_total = len(branches_all)\n\ndef branch_bucket(b):\n    if re.fullmatch(r\"v?\\d+(\\.\\d+){1,3}([.-]\\w+)?\", b):\n        return \"version-branch\"\n    if \"/\" in b:\n        return b.split(\"/\", 1)[0].lower()\n    return \"root\"\n\nbranch_buckets = Counter(branch_bucket(b) for b in branches_all)\ntop_branch_buckets = branch_buckets.most_common(20)\n\nstandard_candidates = [\"main\", \"master\", \"develop\", \"dev\", \"staging\", \"release\", \"releases\", \"hotfix\"]\npresent_standard = [c for c in standard_candidates if c in set(branches_all)]\n\nbranches_sample_latest = branches_all[:60]\nbranches_sample_sorted = sorted(branches_all)[:60]\n\n# --- Tags ---\ntags_all = run_git_command(\"git tag --list\", repo_dir).splitlines()\ntags_all = [t.strip() for t in tags_all if t.strip()]\ntags_total = len(tags_all)\n\ntags_recent = run_git_command(\n    \"git tag --sort=-creatordate --format='%(refname:short) | %(creatordate:short)' | head -n 50\",\n    repo_dir,\n)\ntags_oldest = run_git_command(\n    \"git tag --sort=creatordate --format='%(refname:short) | %(creatordate:short)' | head -n 50\",\n    repo_dir,\n)\n\ndef parse_tag_lines(text):\n    out = []\n    for line in text.splitlines():\n        if \"|\" not in line:\n            continue\n        tag, date_s = [x.strip() for x in line.split(\"|\", 1)]\n        try:\n            dt = datetime.strptime(date_s, \"%Y-%m-%d\")\n        except Exception:\n            continue\n        out.append((tag, dt))\n    return out\n\ntags_recent_parsed = parse_tag_lines(tags_recent)\ntags_oldest_parsed = parse_tag_lines(tags_oldest)\n\ntags_v_dates_raw = run_git_command(\n    r\"git for-each-ref --sort=creatordate --format='%(refname:short) | %(creatordate:short)' refs/tags | grep -E '^v[0-9]+'\",\n    repo_dir,\n)\ntags_docker_dates_raw = run_git_command(\n    r\"git for-each-ref --sort=creatordate --format='%(refname:short) | %(creatordate:short)' refs/tags | grep -E '^docker-rebuild-'\",\n    repo_dir,\n)\n\ndef avg_delta_days(pairs):\n    if len(pairs) < 2:\n        return None\n    deltas = [(pairs[i][1] - pairs[i-1][1]).days for i in range(1, len(pairs))]\n    deltas = [d for d in deltas if d >= 0]\n    if not deltas:\n        return None\n    return sum(deltas) / len(deltas), min(deltas), max(deltas), len(deltas)\n\nv_pairs = parse_tag_lines(tags_v_dates_raw)\ndocker_pairs = parse_tag_lines(tags_docker_dates_raw)\n\nv_stats = avg_delta_days(v_pairs)\ndocker_stats = avg_delta_days(docker_pairs)\n\nraw_commits = run_git_command(\"git log --oneline -n 30\", repo_dir)\nraw_authors = run_git_command(\"git log -n 200 --pretty=format:'%an' | sort | uniq -c | sort -nr | head -n 10\", repo_dir)\n\ncontributing_content = \"Arquivo CONTRIBUTING/CONTRIBUTORS nÃ£o encontrado.\"\ncontributing_path = None\nfor path in glob.glob(\"**/*\", recursive=True):\n    name = os.path.basename(path).lower()\n    if ((name.startswith(\"contributing\") or name.startswith(\"contributors\")) and name.endswith(\".md\") and os.path.isfile(path)):\n        contributing_path = path\n        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            contributing_content = f.read()[:2500]\n        break\n\ndisplay(Markdown(\"## EvidÃªncias coletadas (resumo)\"))\ndisplay(Markdown(f\"- Branches remotos total: **{branches_total}**\"))\ndisplay(Markdown(f\"- Tags total: **{tags_total}**\"))\ndisplay(Markdown(f\"- Branches padrÃ£o encontradas: **{present_standard if present_standard else 'nenhuma das esperadas'}**\"))\ndisplay(Markdown(f\"**Top 20 categorias de branches (por prefixo):**\\n```{top_branch_buckets}\\n```\"))\ndisplay(Markdown(f\"**50 tags mais recentes (tag | data):**\\n```\\n{tags_recent}\\n```\"))\ndisplay(Markdown(f\"**50 tags mais antigas (tag | data):**\\n```\\n{tags_oldest}\\n```\"))\ndisplay(Markdown(f\"**Top autores (Ãºltimos 200 commits):**\\n```\\n{raw_authors}\\n```\"))\nif contributing_path:\n    display(Markdown(f\"**Doc encontrada em `{contributing_path}` (trecho):**\\n```\\n{contributing_content[:800]}...\\n```\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T23:34:56.254207Z","iopub.execute_input":"2025-12-16T23:34:56.254849Z","iopub.status.idle":"2025-12-16T23:35:01.737823Z","shell.execute_reply.started":"2025-12-16T23:34:56.254823Z","shell.execute_reply":"2025-12-16T23:35:01.737260Z"}},"outputs":[{"name":"stdout","text":"Clonando https://github.com/unclecode/crawl4ai.git em /kaggle/working/crawl4ai_activity2 ...\n","output_type":"stream"},{"name":"stderr","text":"Cloning into '/kaggle/working/crawl4ai_activity2'...\n","output_type":"stream"},{"name":"stdout","text":"DiretÃ³rio de trabalho: /kaggle/working/crawl4ai_activity2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## EvidÃªncias coletadas (resumo)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"- Branches remotos total: **147**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"- Tags total: **40**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"- Branches padrÃ£o encontradas: **['main', 'develop', 'staging']**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Top 20 categorias de branches (por prefixo):**\n```[('root', 55), ('fix', 27), ('version-branch', 21), ('feature', 14), ('release', 9), ('codex', 6), ('docker', 3), ('claude', 2), ('feat', 2), ('unclecode', 2), ('bug', 1), ('bugfix', 1), ('coderabbitai', 1), ('copilot', 1), ('devin', 1), ('patch', 1)]\n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**50 tags mais recentes (tag | data):**\n```\ndocker-rebuild-v0.7.8 | 2025-12-11\nv0.7.8 | 2025-12-09\nv0.7.7 | 2025-11-14\ndocker-rebuild-v0.7.7 | 2025-11-14\ndocker-rebuild-v0.7.6 | 2025-10-22\nv0.7.6 | 2025-10-22\ndocker-rebuild-v0.7.5 | 2025-10-21\nv0.7.5 | 2025-10-21\nv0.7.4 | 2025-08-17\nv0.7.3 | 2025-08-09\nv0.7.2 | 2025-07-25\nv0.7.1 | 2025-07-17\nv0.7.0 | 2025-07-12\nvr0.6.3 | 2025-05-12\nv0.6.3 | 2025-05-12\nvr0.6.0 | 2025-04-23\nvr0.6.0rc1 | 2025-04-22\nv0.5.0.post1 | 2025-03-04\nv0.4.243 | 2025-01-01\nv0.4.24 | 2024-12-31\nv0.3.746 | 2024-11-29\nv0.3.745 | 2024-11-28\nv.3.72 | 2024-10-27\nv0.3.6 | 2024-10-14\n0.3.4 | 2024-09-29\nv0.3.3 | 2024-09-29\nv0.3.0 | 2024-09-28\nv0.2.77 | 2024-08-04\nv0.0.75 | 2024-07-19\nv0.2.74 | 2024-07-08\nv0.2.73 | 2024-07-03\nv0.2.72 | 2024-06-30\nv0.2.71 | 2024-06-26\nv0.2.7 | 2024-06-24\nv0.2.6 | 2024-06-22\nv0.2.4 | 2024-06-17\nv0.2.2 | 2024-05-19\nv0.2.1 | 2024-05-19\nv0.2.0 | 2024-05-17\nv0.1.0 | 2024-05-17\n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**50 tags mais antigas (tag | data):**\n```\nv0.1.0 | 2024-05-17\nv0.2.0 | 2024-05-17\nv0.2.1 | 2024-05-19\nv0.2.2 | 2024-05-19\nv0.2.4 | 2024-06-17\nv0.2.6 | 2024-06-22\nv0.2.7 | 2024-06-24\nv0.2.71 | 2024-06-26\nv0.2.72 | 2024-06-30\nv0.2.73 | 2024-07-03\nv0.2.74 | 2024-07-08\nv0.0.75 | 2024-07-19\nv0.2.77 | 2024-08-04\nv0.3.0 | 2024-09-28\nv0.3.3 | 2024-09-29\n0.3.4 | 2024-09-29\nv0.3.6 | 2024-10-14\nv.3.72 | 2024-10-27\nv0.3.745 | 2024-11-28\nv0.3.746 | 2024-11-29\nv0.4.24 | 2024-12-31\nv0.4.243 | 2025-01-01\nv0.5.0.post1 | 2025-03-04\nvr0.6.0rc1 | 2025-04-22\nvr0.6.0 | 2025-04-23\nv0.6.3 | 2025-05-12\nvr0.6.3 | 2025-05-12\nv0.7.0 | 2025-07-12\nv0.7.1 | 2025-07-17\nv0.7.2 | 2025-07-25\nv0.7.3 | 2025-08-09\nv0.7.4 | 2025-08-17\ndocker-rebuild-v0.7.5 | 2025-10-21\nv0.7.5 | 2025-10-21\ndocker-rebuild-v0.7.6 | 2025-10-22\nv0.7.6 | 2025-10-22\ndocker-rebuild-v0.7.7 | 2025-11-14\nv0.7.7 | 2025-11-14\nv0.7.8 | 2025-12-09\ndocker-rebuild-v0.7.8 | 2025-12-11\n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Top autores (Ãºltimos 200 commits):**\n```\n62 ntohidi\n     45 Nasrin\n     26 unclecode\n     14 AHMET YILMAZ\n     12 Soham Kukreti\n     12 Aravind Karnam\n      8 UncleCode\n      7 Claude\n      4 Aravind\n      3 prokopis3\n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Doc encontrada em `CONTRIBUTORS.md` (trecho):**\n```\n# Contributors to Crawl4AI\n\nWe would like to thank the following people for their contributions to Crawl4AI:\n\n## Core Team\n\n- [Unclecode](https://github.com/unclecode) - Project Creator and Main Developer\n- [Nasrin](https://github.com/ntohidi) - Project Manager and Developer\n- [Aravind Karnam](https://github.com/aravindkarnam) - Head of Community and Product \n\n## Community Contributors\n\n- [aadityakanjolia4](https://github.com/aadityakanjolia4) - Fix for `CustomHTML2Text` is not defined.\n- [FractalMind](https://github.com/FractalMind) - Created the first official Docker Hub image and fixed Dockerfile errors\n- [ketonkss4](https://github.com/ketonkss4) - Identified Selenium's new capabilities, helping reduce dependencies\n- [jonymusky](https://github.com/jonymusky) - Javascript execution docum...\n```"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"## CÃ‰LULA 3: ANÃLISE MAIS EMBASADA (com mÃ©tricas + evidÃªncias) (Defasado)\n\nv_stats_text = \"N/A\"\nif v_stats:\n    avg_, min_, max_, n_ = v_stats\n    v_stats_text = f\"v*: mÃ©dia {avg_:.2f} dias (min {min_}, max {max_}, n={n_})\"\n\ndocker_stats_text = \"N/A\"\nif docker_stats:\n    avg_, min_, max_, n_ = docker_stats\n    docker_stats_text = f\"docker-rebuild-*: mÃ©dia {avg_:.2f} dias (min {min_}, max {max_}, n={n_})\"\n\nprompt_branching = f\"\"\"\nVocÃª Ã© um especialista em DevOps e Engenharia de Software.\n\nREGRAS IMPORTANTES:\n- NÃƒO invente branches que nÃ£o estejam explicitamente nos dados.\n- Se faltar evidÃªncia, diga \"evidÃªncia insuficiente\" e explique o que faltou.\n- Use contagens, padrÃµes de nomes e exemplos concretos.\n\nDADOS (EVIDÃŠNCIAS):\n1) Total de branches remotas: {branches_total}\n2) Branches \"padrÃ£o\" encontradas entre [main, master, develop, dev, staging, release, releases, hotfix]:\n{present_standard}\n\n3) Top categorias de branches por prefixo (prefixo -> contagem):\n{top_branch_buckets}\n\n4) Amostra (ordenada) de branches (primeiras 60 em ordem alfabÃ©tica):\n{branches_sample_sorted}\n\n5) Trecho de documento de contribuiÃ§Ã£o (se existir):\nCaminho: {contributing_path if contributing_path else \"N/A\"}\nConteÃºdo:\n{contributing_content}\n\nTAREFA:\n1) Classifique o \"Modelo de Fluxo de Trabalho\" do projeto, escolhendo UMA:\nA) Gitflow\nB) GitHub Flow\nC) Trunk-Based Development\n\n2) Justifique com pelo menos 8 evidÃªncias objetivas, citando nomes/padrÃµes de branches e as contagens.\n3) Se houver mistura de modelos (ex: branches de versÃ£o + fluxo por PR), diga isso e explique.\n\nResponda em PT-BR com seÃ§Ãµes: \"ClassificaÃ§Ã£o\", \"EvidÃªncias\", \"Riscos/ObservaÃ§Ãµes\".\n\"\"\"\n\nprint(\"--- 1) AnÃ¡lise: Branching Model ---\")\nanalysis_branching = ask_codellama(prompt_branching, max_new_tokens=900)\ndisplay(Markdown(\"## ğŸŒ Modelo de Fluxo de Trabalho (Branching Model)\"))\ndisplay(Markdown(analysis_branching))\n\nprompt_releases = f\"\"\"\nVocÃª Ã© um especialista em GovernanÃ§a de Software.\n\nREGRAS IMPORTANTES:\n- NÃƒO conclua \"Release Train\" sÃ³ por existir semÃ¢ntica de versÃµes.\n- Use datas e intervalos para sustentar o argumento.\n- Se houver mais de um tipo de tag (ex: v* e docker-rebuild-*), analise separadamente.\n\nDADOS (EVIDÃŠNCIAS):\n1) Total de tags: {tags_total}\n\n2) 50 tags mais antigas (tag | data):\n{tags_oldest}\n\n3) 50 tags mais recentes (tag | data):\n{tags_recent}\n\n4) EstatÃ­sticas de frequÃªncia:\n- {v_stats_text}\n- {docker_stats_text}\n\n5) Ãšltimos 30 commits (hash+mensagem):\n{raw_commits}\n\n\nTAREFA:\nClassifique a estratÃ©gia de releases escolhendo UMA (ou \"Mista\", se justificar claramente):\n- LTS + Current\n- Release Train\n- Rapid Release / Continuous Delivery\n- Ad-hoc\n- Mista\n\nExija evidÃªncias: cite datas, intervalos (mÃ©dia/min/max) e padrÃ£o de tags.\nResponda em PT-BR com seÃ§Ãµes: \"ClassificaÃ§Ã£o\", \"EvidÃªncias\", \"ConclusÃ£o\".\n\"\"\"\n\nprint(\"\\n--- 2) AnÃ¡lise: EstratÃ©gia de Releases ---\")\nanalysis_releases = ask_codellama(prompt_releases, max_new_tokens=900)\n\ndisplay(Markdown(\"## â±ï¸ EstratÃ©gia de Releases (Ritmo de Entrega)\"))\ndisplay(Markdown(analysis_releases))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T23:35:04.876527Z","iopub.execute_input":"2025-12-16T23:35:04.876817Z","iopub.status.idle":"2025-12-16T23:36:35.831823Z","shell.execute_reply.started":"2025-12-16T23:35:04.876796Z","shell.execute_reply":"2025-12-16T23:36:35.831119Z"}},"outputs":[{"name":"stdout","text":"--- 1) AnÃ¡lise: Branching Model ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## ğŸŒ Modelo de Fluxo de Trabalho (Branching Model)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"ClassificaÃ§Ã£o:\n\nCom base nos dados fornecidos, pode-se concluir que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n\nEvidÃªncias:\n\n1. A maioria das branches remotas encontradas sÃ£o branches de desenvolvimento, como \"main\", \"master\", \"develop\", \"dev\", \"staging\", \"release\", \"releases\", e \"hotfix\". Isso sugiere que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n2. A contagem de branches \"padrÃ£o\" encontradas entre [main, master, develop, dev, staging, release, releases, hotfix] Ã© de 55 branches. Isso sugiere que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n3. A contagem de branches por prefixo (prefixo -> contagem) Ã©:\n* root: 55\n* fix: 27\n* version-branch: 21\n* feature: 14\n* release: 9\n* codex: 6\n* docker: 3\n* claude: 2\n* feat: 2\n* unclecode: 2\n* bug: 1\n* bugfix: 1\n* coderabbitai: 1\n* copilot: 1\n* devin: 1\n* patch: 1\n\nEssas contagens sugerem que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n\nRiscos/ObservaÃ§Ãµes:\n\n1. A presenÃ§a de branches de desenvolvimento, como \"develop\" e \"dev\", pode indicar que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n2. A presenÃ§a de branches de versÃ£o, como \"release\" e \"releases\", pode indicar que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n3. A presenÃ§a de branches de hotfix, como \"hotfix\", pode indicar que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n4. A presenÃ§a de branches de desenvolvimento, como \"main\" e \"master\", pode indicar que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n5. A presenÃ§a de branches de cÃ³digo, como \"codex\", pode indicar que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n6. A presenÃ§a de branches de Docker, como \"docker\", pode indicar que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n7. A presenÃ§a de branches de claude, como \"claude\", pode indicar que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n8. A presenÃ§a de branches de feat, como \"feat\", pode indicar que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n9. A presenÃ§a de branches de unclecode, como \"unclecode\", pode indicar que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n10. A presenÃ§a de branches de bug, como \"bug\", pode indicar que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n11. A presenÃ§a de branches de bugfix, como \"bugfix\", pode indicar que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n12. A presenÃ§a de branches de coderabbitai, como \"coderabbitai\", pode indicar que o projeto utiliza um modelo de fluxo de trabalho baseado em Gitflow.\n13. A presenÃ§a de branches de copilot, como \"copilot\", pode indicar que o projeto utiliza um modelo de fluxo de trabal"},"metadata":{}},{"name":"stdout","text":"\n--- 2) AnÃ¡lise: EstratÃ©gia de Releases ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## â±ï¸ EstratÃ©gia de Releases (Ritmo de Entrega)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"ClassificaÃ§Ã£o: LTS + Current\n\nEvidÃªncias:\n\n* A mÃ©dia de intervalos entre as tags Ã© de aproximadamente 19 dias, o que indica que as releases sÃ£o feitas com uma frequÃªncia moderada.\n* A mÃ©dia de intervalos entre as tags de \"docker-rebuild-*\" Ã© de aproximadamente 17 dias, o que indica que as releases sÃ£o feitas com uma frequÃªncia moderada.\n* A mÃ©dia de intervalos entre as tags de \"v*\" Ã© de aproximadamente 19 dias, o que indica que as releases sÃ£o feitas com uma frequÃªncia moderada.\n* A Ãºltima tag de \"v*\" foi criada em 2025-12-09, enquanto a Ãºltima tag de \"docker-rebuild-*\" foi criada em 2025-12-11, o que indica que as releases sÃ£o feitas com uma frequÃªncia moderada.\n* A Ãºltima tag de \"v*\" Ã© uma release de longo prazo, enquanto as tags de \"docker-rebuild-*\" sÃ£o releases de recompilaÃ§Ã£o, o que indica que a estratÃ©gia de releases Ã© LTS + Current.\n\nConclusÃ£o:\nCom base nas evidÃªncias fornecidas, a estratÃ©gia de releases escolhida Ã© LTS + Current. A estratÃ©gia LTS + Current Ã© uma estratÃ©gia de releases que combina a estabilidade de uma release de longo prazo com a frequÃªncia de releases de recompilaÃ§Ã£o. A estratÃ©gia Ã© usada por muitos projetos de software, incluindo o prÃ³prio GitHub, para garantir que as releases estejam atualizadas e seguras. A estratÃ©gia LTS + Current Ã© uma boa opÃ§Ã£o para o projeto, pois permite que as releases sejam feitas com uma frequÃªncia moderada, enquanto ainda oferece a estabilidade e a seguranÃ§a de uma release de longo prazo."},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"## CÃ‰LULA 4: EVIDÃŠNCIAS FORTES (merge-base, atividade, relaÃ§Ã£o entre branches)\n\nimport re\nfrom collections import defaultdict\n\ndef safe(cmd):\n    return run_git_command(cmd, repo_dir)\n\n# Branches-chave (ajuste se quiser)\nkey_branches = [\"main\", \"develop\", \"staging\"]\nkey_branches_present = [b for b in key_branches if b in branches_all]\n\n# 1) Quantos commits exclusivos existem em develop vs main?\nbranch_ahead_behind = {}\nfor b in key_branches_present:\n    if b == \"main\":\n        continue\n    out = safe(f\"git rev-list --left-right --count origin/main...origin/{b}\")\n    # formato: \"<left> <right>\" (left=main-only, right=branch-only) depende da ordem\n    # aqui usamos main...branch => left=main_only, right=branch_only\n    try:\n        main_only, branch_only = map(int, out.split())\n        branch_ahead_behind[b] = {\"main_only\": main_only, \"branch_only\": branch_only}\n    except:\n        branch_ahead_behind[b] = {\"error\": out}\n\n# 2) Merge-base do release/* com main e develop (indica de onde sai o release)\nrelease_branches = [b for b in branches_all if b.startswith(\"release/\")]\nrelease_mergebase = []\nfor rb in sorted(release_branches)[:30]:  # amostra\n    mb_main = safe(f\"git merge-base origin/main origin/{rb}\")\n    mb_dev = safe(f\"git merge-base origin/develop origin/{rb}\") if \"develop\" in branches_all else \"N/A\"\n    release_mergebase.append((rb, mb_main[:8], mb_dev[:8] if mb_dev != \"N/A\" else \"N/A\"))\n\n# 3) Ãšltimos merges no main (padrÃ£o de PR-based)\nmerges_main = safe(\"git log origin/main --merges -n 30 --pretty=format:'%h | %ad | %s' --date=short\")\nmerges_develop = safe(\"git log origin/develop --merges -n 30 --pretty=format:'%h | %ad | %s' --date=short\") if \"develop\" in branches_all else \"N/A\"\n\n# 4) Onde as tags v* apontam (commitish)\ntags_v = [t for t in tags_all if re.match(r\"^v\\d\", t)]\ntag_targets = []\nfor t in tags_v[-20:]:\n    target = safe(f\"git rev-list -n 1 {t}\")\n    tag_targets.append((t, target[:8]))\n\nprint(\"Branches-chave presentes:\", key_branches_present)\nprint(\"\\nAhead/Behind (main...branch):\", branch_ahead_behind)\nprint(\"\\nAmostra merge-base release/* (rb, mb_main, mb_develop):\")\nfor row in release_mergebase:\n    print(\"  \", row)\nprint(\"\\nÃšltimos merges (main):\\n\", merges_main)\nprint(\"\\nÃšltimos merges (develop):\\n\", merges_develop)\nprint(\"\\nTargets (Ãºltimas 20 tags v*):\")\nfor row in tag_targets:\n    print(\"  \", row)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T23:36:41.646072Z","iopub.execute_input":"2025-12-16T23:36:41.646568Z","iopub.status.idle":"2025-12-16T23:36:41.869061Z","shell.execute_reply.started":"2025-12-16T23:36:41.646544Z","shell.execute_reply":"2025-12-16T23:36:41.868367Z"}},"outputs":[{"name":"stdout","text":"Branches-chave presentes: ['main', 'develop', 'staging']\n\nAhead/Behind (main...branch): {'develop': {'main_only': 0, 'branch_only': 33}, 'staging': {'main_only': 978, 'branch_only': 0}}\n\nAmostra merge-base release/* (rb, mb_main, mb_develop):\n   ('release/v0.7.0', '0c8bb742', '0c8bb742')\n   ('release/v0.7.1', '9f9ea3bb', '9f9ea3bb')\n   ('release/v0.7.2', '48647300', '48647300')\n   ('release/v0.7.3', 'a9a2d798', 'a9a2d798')\n   ('release/v0.7.4', '5398acc7', '5398acc7')\n   ('release/v0.7.5', 'f6a02c43', 'f6a02c43')\n   ('release/v0.7.6', '7e8fb3a8', '7e8fb3a8')\n   ('release/v0.7.7', '6244f56f', '6244f56f')\n   ('release/v0.7.8', '3a07c596', '5a8fb577')\n\nÃšltimos merges (main):\n cb637fb | 2025-11-16 | Merge pull request #1613 from unclecode/release/v0.7.7\n2c973b1 | 2025-11-13 | Merge branch 'develop' into release/v0.7.7\nf3146de | 2025-11-13 | Merge pull request #1609 from unclecode/fix/update-config-documentation\n466be69 | 2025-11-13 | Merge pull request #1607 from unclecode/fix/dfs_deep_crawling\n8116b15 | 2025-11-13 | Merge pull request #1596 from unclecode/docs-proxy-security\n89cc29f | 2025-11-12 | Merge branch 'fix/docker' into develop\ncdcb883 | 2025-11-12 | Merge pull request #1605 from Nstproxy/feat/nstproxy\nb207ae2 | 2025-11-12 | Merge pull request #1528 from unclecode/fix/managed-browser-cdp-timing\nbe00fc3 | 2025-11-12 | Merge pull request #1598 from unclecode/fix/sitemap_seeder\n124ac58 | 2025-11-12 | Merge pull request #1599 from unclecode/docs-llm-strategies-update\na99cd37 | 2025-11-11 | Merge pull request #1597 from unclecode/sponsors/capsolver\n006e29f | 2025-11-10 | Merge pull request #1589 from capsolver/main\nd56b0eb | 2025-11-06 | Merge pull request #1495 from unclecode/fix/viewport_in_managed_browser\n66175e1 | 2025-11-06 | Merge pull request #1590 from unclecode/fix/async-llm-extraction-arunMany\n2c91815 | 2025-11-06 | Merge pull request #1529 from unclecode/fix/remove_overlay_elements\n854694e | 2025-11-06 | Merge pull request #1537 from unclecode/fix/docker-compose-llm-env\n6534ece | 2025-11-05 | Merge pull request #1532 from unclecode/fix/update-documentation\n89e28d4 | 2025-10-28 | Merge pull request #1558 from unclecode/claude/fix-update-pyopenssl-security-011CUPexU25DkNvoxfu5ZrnB\n4df8389 | 2025-10-23 | Merge pull request #1560 from unclecode/fix/marketplace\n6aef5a1 | 2025-10-22 | Merge branch 'main' into develop\n7e8fb3a | 2025-10-22 | Merge branch 'release/v0.7.5' into develop\nb71d624 | 2025-10-22 | Merge branch 'implement-webhook-crawl-feature-011CULZY1Jy8N5MUkZqXkRVp' into develop\nf6a02c4 | 2025-10-21 | Merge branch 'develop' into release/v0.7.5\nfdbcddb | 2025-10-17 | Merge pull request #1546 from unclecode/sponsors\n7dadb65 | 2025-10-13 | Merge branch 'develop' into release/v0.7.5\n611d48f | 2025-10-09 | Merge branch 'develop' into release/v0.7.5\n936397e | 2025-10-09 | Merge branch 'develop' of https://github.com/unclecode/crawl4ai into develop\n9900f63 | 2025-10-03 | Merge pull request #1531 from unclecode/develop\n9292b26 | 2025-10-03 | Merge branch 'develop' of https://github.com/unclecode/crawl4ai into develop\n80aa6c1 | 2025-10-03 | Merge pull request #1530 from Sjoeborg/fix/arun-many-returns-none\n\nÃšltimos merges (develop):\n 306ddcb | 2025-12-11 | Merge branch 'main' into develop\n5a8fb57 | 2025-12-03 | Merge pull request #1648 from christopher-w-murphy/fix/content-relevance-filter\nf32cfc6 | 2025-12-02 | Merge pull request #1645 from unclecode/fix/configurable-backoff\nd06c39e | 2025-12-02 | Merge pull request #1641 from unclecode/fix/serialize-proxy-config\nafc31e1 | 2025-12-02 | Merge branch 'develop' of https://github.com/unclecode/crawl4ai into develop\ndcb77c9 | 2025-11-27 | Merge pull request #1623 from unclecode/fix/deprecated_pydantic\n94c8a83 | 2025-11-25 | Merge pull request #1447 from rbushri/fix/wrong_url_raw\n7771ed3 | 2025-11-24 | Merge branch 'develop' into fix/wrong_url_raw\ncb637fb | 2025-11-16 | Merge pull request #1613 from unclecode/release/v0.7.7\n2c973b1 | 2025-11-13 | Merge branch 'develop' into release/v0.7.7\nf3146de | 2025-11-13 | Merge pull request #1609 from unclecode/fix/update-config-documentation\n466be69 | 2025-11-13 | Merge pull request #1607 from unclecode/fix/dfs_deep_crawling\n8116b15 | 2025-11-13 | Merge pull request #1596 from unclecode/docs-proxy-security\n89cc29f | 2025-11-12 | Merge branch 'fix/docker' into develop\ncdcb883 | 2025-11-12 | Merge pull request #1605 from Nstproxy/feat/nstproxy\nb207ae2 | 2025-11-12 | Merge pull request #1528 from unclecode/fix/managed-browser-cdp-timing\nbe00fc3 | 2025-11-12 | Merge pull request #1598 from unclecode/fix/sitemap_seeder\n124ac58 | 2025-11-12 | Merge pull request #1599 from unclecode/docs-llm-strategies-update\na99cd37 | 2025-11-11 | Merge pull request #1597 from unclecode/sponsors/capsolver\n006e29f | 2025-11-10 | Merge pull request #1589 from capsolver/main\nd56b0eb | 2025-11-06 | Merge pull request #1495 from unclecode/fix/viewport_in_managed_browser\n66175e1 | 2025-11-06 | Merge pull request #1590 from unclecode/fix/async-llm-extraction-arunMany\n2c91815 | 2025-11-06 | Merge pull request #1529 from unclecode/fix/remove_overlay_elements\n854694e | 2025-11-06 | Merge pull request #1537 from unclecode/fix/docker-compose-llm-env\n6534ece | 2025-11-05 | Merge pull request #1532 from unclecode/fix/update-documentation\n89e28d4 | 2025-10-28 | Merge pull request #1558 from unclecode/claude/fix-update-pyopenssl-security-011CUPexU25DkNvoxfu5ZrnB\n4df8389 | 2025-10-23 | Merge pull request #1560 from unclecode/fix/marketplace\n6aef5a1 | 2025-10-22 | Merge branch 'main' into develop\n7e8fb3a | 2025-10-22 | Merge branch 'release/v0.7.5' into develop\nb71d624 | 2025-10-22 | Merge branch 'implement-webhook-crawl-feature-011CULZY1Jy8N5MUkZqXkRVp' into develop\n\nTargets (Ãºltimas 20 tags v*):\n   ('v0.2.74', '4d283ab3')\n   ('v0.2.77', 'e5e6a34e')\n   ('v0.3.0', '5d4e92db')\n   ('v0.3.3', '0759503e')\n   ('v0.3.6', '9ffa34b6')\n   ('v0.3.745', 'c8485776')\n   ('v0.3.746', '8c76a8c7')\n   ('v0.4.24', '67d0999b')\n   ('v0.4.243', '318554e6')\n   ('v0.5.0.post1', '9c58e4ce')\n   ('v0.6.3', '897e0173')\n   ('v0.7.0', '7b9ba301')\n   ('v0.7.1', '0163bd79')\n   ('v0.7.2', '48647300')\n   ('v0.7.3', '21f79fe1')\n   ('v0.7.4', 'e651e045')\n   ('v0.7.5', 'f6a02c43')\n   ('v0.7.6', 'ca100c65')\n   ('v0.7.7', '6244f56f')\n   ('v0.7.8', '60d61739')\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"## CÃ‰LULA 5: PROMPT MELHORADO (Branching Model) - sem inventar nÃºmeros/branches\n\n# Monte uma lista curta e objetiva de branches relevantes para o modelo\nbranches_explicit = {\n    \"default_branch\": run_git_command(\"git remote show origin | sed -n '/HEAD branch/s/.*: //p'\", repo_dir),\n    \"standard_present\": present_standard,\n    \"release_branches_sample\": sorted([b for b in branches_all if b.startswith(\"release/\")])[:30],\n    \"feature_branches_sample\": sorted([b for b in branches_all if b.startswith(\"feature/\")])[:30],\n    \"fix_branches_sample\": sorted([b for b in branches_all if b.startswith(\"fix/\")])[:30],\n    \"version_branches_sample\": sorted([b for b in branches_all if re.fullmatch(r\"v?\\d+(\\.\\d+){1,3}([.-]\\w+)?\", b)])[:30],\n    \"main_variants_sample\": sorted([b for b in branches_all if b.startswith(\"main-\") or b.startswith(\"main_\") or b.startswith(\"mainv\") or b.startswith(\"main-v\")])[:30],\n}\n\nprompt_branching = f\"\"\"\nVocÃª Ã© um especialista em branching models (GitFlow, GitHub Flow, Trunk-Based).\n\nREGRAS (OBRIGATÃ“RIAS):\n1) VocÃª SÃ“ pode usar branches/contagens que aparecem explicitamente nos DADOS abaixo.\n2) Ã‰ PROIBIDO citar branches que nÃ£o estejam nos DADOS (ex: 'master' se nÃ£o estiver).\n3) Ã‰ PROIBIDO inventar contagens como \"padrÃ£o vs custom\" se eu nÃ£o fornecer esses nÃºmeros.\n4) Se faltarem evidÃªncias para diferenciar GitHub Flow vs Trunk-Based, responda \"EvidÃªncia insuficiente\" e diga exatamente o que faltou.\n\nDADOS:\n- Total de branches remotas: {branches_total}\n- Default branch (origin HEAD): {branches_explicit[\"default_branch\"]}\n- Branches padrÃ£o encontradas: {branches_explicit[\"standard_present\"]}\n\n- DistribuiÃ§Ã£o por categoria (prefixo -> contagem):\n{top_branch_buckets}\n\n- Amostras (listas reais):\nrelease/* sample: {branches_explicit[\"release_branches_sample\"]}\nfeature/* sample: {branches_explicit[\"feature_branches_sample\"]}\nfix/* sample: {branches_explicit[\"fix_branches_sample\"]}\nversion-like sample: {branches_explicit[\"version_branches_sample\"]}\nmain-variants sample: {branches_explicit[\"main_variants_sample\"]}\n\nEVIDÃŠNCIAS DE FLUXO (texto bruto):\n- Ãšltimos merges no main:\n{merges_main}\n\n- RelaÃ§Ã£o main...develop (se existir):\n{branch_ahead_behind}\n\nTAREFA:\n1) Escolha UMA classificaÃ§Ã£o:\nA) GitFlow\nB) GitHub Flow\nC) Trunk-Based Development\n2) Liste exatamente 8 a 12 evidÃªncias objetivas (com nomes reais).\n3) Se houver mistura (ex.: trunk + release/*), diga explicitamente \"Misto\", mas ainda escolha a opÃ§Ã£o mais prÃ³xima.\n4) NÃ£o use emojis.\n\nFormato:\n- ClassificaÃ§Ã£o:\n- EvidÃªncias:\n- Riscos/ObservaÃ§Ãµes:\n\"\"\"\n\nanalysis_branching = ask_codellama(prompt_branching, max_new_tokens=900)\ndisplay(Markdown(\"## ğŸŒ Modelo de Fluxo de Trabalho (Branching Model) â€” Melhorado\"))\ndisplay(Markdown(analysis_branching))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T23:36:47.004331Z","iopub.execute_input":"2025-12-16T23:36:47.004603Z","iopub.status.idle":"2025-12-16T23:37:53.792780Z","shell.execute_reply.started":"2025-12-16T23:36:47.004583Z","shell.execute_reply":"2025-12-16T23:37:53.792212Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## ğŸŒ Modelo de Fluxo de Trabalho (Branching Model) â€” Melhorado"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"ClassificaÃ§Ã£o: Trunk-Based Development\n\nEvidÃªncias:\n\n1. A branch padrÃ£o Ã© \"main\".\n2. A branch \"develop\" Ã© uma branch de desenvolvimento.\n3. A branch \"staging\" Ã© uma branch de preparaÃ§Ã£o.\n4. A branch \"release/*\" Ã© uma branch de liberaÃ§Ã£o.\n5. A branch \"fix/*\" Ã© uma branch de correÃ§Ã£o.\n6. A branch \"version-like\" Ã© uma branch de versÃ£o.\n7. A branch \"main-variants\" Ã© uma branch de variantes do main.\n\nRiscos/ObservaÃ§Ãµes:\n\n* A branch \"develop\" Ã© uma branch de desenvolvimento e Ã© frequentemente mesclada com a branch \"main\". Isso sugere que o fluxo de trabalho Ã© baseado em Trunk-Based Development.\n* A branch \"release/*\" Ã© uma branch de liberaÃ§Ã£o e Ã© frequentemente mesclada com a branch \"main\". Isso sugere que o fluxo de trabalho Ã© baseado em Trunk-Based Development.\n* A branch \"fix/*\" Ã© uma branch de correÃ§Ã£o e Ã© frequentemente mesclada com a branch \"main\". Isso sugere que o fluxo de trabalho Ã© baseado em Trunk-Based Development.\n* A branch \"version-like\" Ã© uma branch de versÃ£o e Ã© frequentemente mesclada com a branch \"main\". Isso sugere que o fluxo de trabalho Ã© baseado em Trunk-Based Development.\n* A branch \"main-variants\" Ã© uma branch de variantes do main e Ã© frequentemente mesclada com a branch \"main\". Isso sugere que o fluxo de trabalho Ã© baseado em Trunk-Based Development.\n\nObservaÃ§Ãµes:\n\n* A branch \"root\" nÃ£o Ã© uma branch padrÃ£o e nÃ£o Ã© usada frequentemente no fluxo de trabalho.\n* A branch \"codex\" nÃ£o Ã© uma branch padrÃ£o e nÃ£o Ã© usada frequentemente no fluxo de trabalho.\n* A branch \"docker\" nÃ£o Ã© uma branch padrÃ£o e nÃ£o Ã© usada frequentemente no fluxo de trabalho.\n* A branch \"claude\" nÃ£o Ã© uma branch padrÃ£o e nÃ£o Ã© usada frequentemente no fluxo de trabalho.\n* A branch \"feat\" nÃ£o Ã© uma branch padrÃ£o e nÃ£o Ã© usada frequentemente no fluxo de trabalho.\n* A branch \"unclecode\" nÃ£o Ã© uma branch padrÃ£o e nÃ£o Ã© usada frequentemente no fluxo de trabalho.\n* A branch \"bug\" nÃ£o Ã© uma branch padrÃ£o e nÃ£o Ã© usada frequentemente no fluxo de trabalho.\n* A branch \"bugfix\" nÃ£o Ã© uma branch padrÃ£o e nÃ£o Ã© usada frequentemente no fluxo de trabalho.\n* A branch \"coderabbitai\" nÃ£o Ã© uma branch padrÃ£o e nÃ£o Ã© usada frequentemente no fluxo de trabalho.\n* A branch \"copilot\" nÃ£o Ã© uma branch padrÃ£o e nÃ£o Ã© usada frequentemente no fluxo de trabalho.\n* A branch \"devin\" nÃ£o Ã© uma branch padrÃ£o e nÃ£o Ã© usada frequentemente no fluxo de trabalho.\n* A branch \"patch\" nÃ£o Ã© uma branch padrÃ£o e nÃ£o Ã© usada frequentemente no fluxo de trabalho.\n\nEvidencias:\n\n* A branch \"release/v0.7.0\" Ã© uma branch de liberaÃ§Ã£o.\n* A branch \"release/v0.7.1\" Ã© uma branch de liberaÃ§Ã£o.\n* A branch \"release/v0.7.2\" Ã© uma branch de liberaÃ§Ã£o.\n* A branch \"release/v0.7.3\" Ã© uma branch de liberaÃ§Ã£o.\n* A branch \"release/v0.7.4\" Ã© uma branch de liberaÃ§Ã£o.\n* A branch \"release/v0.7.5\" Ã© uma branch de liberaÃ§Ã£o.\n* A branch \"release/v0.7.6\" Ã© uma"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"## CÃ‰LULA 6: MÃ‰TRICAS DE RELEASE MAIS CORRETAS (separando famÃ­lias de tags)\n\nimport re\nfrom statistics import median\n\ndef pairs_from_pattern(pattern):\n    raw = run_git_command(\n        rf\"git for-each-ref --sort=creatordate --format='%(refname:short) | %(creatordate:short)' refs/tags | grep -E '{pattern}'\",\n        repo_dir,\n    )\n    return parse_tag_lines(raw)\n\ndef stats(pairs):\n    if len(pairs) < 2:\n        return None\n    deltas = [(pairs[i][1] - pairs[i-1][1]).days for i in range(1, len(pairs))]\n    deltas = [d for d in deltas if d >= 0]\n    if not deltas:\n        return None\n    return {\n        \"count_tags\": len(pairs),\n        \"count_intervals\": len(deltas),\n        \"avg_days\": sum(deltas)/len(deltas),\n        \"median_days\": median(deltas),\n        \"min_days\": min(deltas),\n        \"max_days\": max(deltas),\n    }\n\npairs_v = pairs_from_pattern(r\"^v[0-9]\")\npairs_docker = pairs_from_pattern(r\"^docker-rebuild-\")\npairs_weird = pairs_from_pattern(r\"^(vr|v\\.[0-9]|[0-9]+\\.)\")\n\nprint(\"Stats v*:\", stats(pairs_v))\nprint(\"Stats docker-rebuild-*:\", stats(pairs_docker))\nprint(\"Stats weird/mixed:\", stats(pairs_weird))\n\nprint(\"\\nAmostra weird tags:\")\nfor t in pairs_weird[:15]:\n    print(\" \", t[0], t[1].date())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T23:37:53.818620Z","iopub.execute_input":"2025-12-16T23:37:53.818857Z","iopub.status.idle":"2025-12-16T23:37:53.840986Z","shell.execute_reply.started":"2025-12-16T23:37:53.818831Z","shell.execute_reply":"2025-12-16T23:37:53.840264Z"}},"outputs":[{"name":"stdout","text":"Stats v*: {'count_tags': 31, 'count_intervals': 30, 'avg_days': 19.033333333333335, 'median_days': 8.0, 'min_days': 0, 'max_days': 69}\nStats docker-rebuild-*: {'count_tags': 4, 'count_intervals': 3, 'avg_days': 17.0, 'median_days': 23, 'min_days': 1, 'max_days': 27}\nStats weird/mixed: {'count_tags': 5, 'count_intervals': 4, 'avg_days': 56.25, 'median_days': 23.5, 'min_days': 1, 'max_days': 177}\n\nAmostra weird tags:\n  0.3.4 2024-09-29\n  v.3.72 2024-10-27\n  vr0.6.0rc1 2025-04-22\n  vr0.6.0 2025-04-23\n  vr0.6.3 2025-05-12\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"## CÃ‰LULA 7: PROMPT MELHORADO (Releases) - exige nÃºmeros calculados\n\nv_stats2 = stats(pairs_v)\ndocker_stats2 = stats(pairs_docker)\nweird_stats2 = stats(pairs_weird)\n\nprompt_releases = f\"\"\"\nVocÃª Ã© especialista em Release Management.\n\nREGRAS (OBRIGATÃ“RIAS):\n1) VocÃª SÃ“ pode usar nÃºmeros presentes nos DADOS abaixo.\n2) Ã‰ PROIBIDO inventar mÃ©dias (ex: \"19 dias\") se eu nÃ£o fornecer.\n3) VocÃª deve separar a anÃ¡lise entre tags v* e docker-rebuild-*.\n\nDADOS:\n- Total de tags (todas): {tags_total}\n\n- EstatÃ­sticas tags v* (calculadas):\n{v_stats2}\n\n- EstatÃ­sticas tags docker-rebuild-* (calculadas):\n{docker_stats2}\n\n- EstatÃ­sticas tags \"weird/mistas\" (vr*, v.* ou sem v):\n{weird_stats2}\n\n- Amostra de tags recentes (tag | data):\n{tags_recent}\n\nTAREFA:\nClassifique a estratÃ©gia de releases escolhendo UMA (ou \"Mista\" se justificar):\n- LTS + Current\n- Release Train\n- Rapid Release / Continuous Delivery\n- Ad-hoc\n- Mista\n\nResponda com:\n- ClassificaÃ§Ã£o\n- EvidÃªncias (6 a 10 bullets)\n- ConclusÃ£o\n\nNÃ£o use emojis.\n\"\"\"\n\nanalysis_releases = ask_codellama(prompt_releases, max_new_tokens=900)\ndisplay(Markdown(\"## â±ï¸ EstratÃ©gia de Releases â€” Melhorado\"))\ndisplay(Markdown(analysis_releases))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T23:38:22.340726Z","iopub.execute_input":"2025-12-16T23:38:22.341472Z","iopub.status.idle":"2025-12-16T23:38:59.785324Z","shell.execute_reply.started":"2025-12-16T23:38:22.341439Z","shell.execute_reply":"2025-12-16T23:38:59.784737Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## â±ï¸ EstratÃ©gia de Releases â€” Melhorado"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"ClassificaÃ§Ã£o: LTS + Current\n\nEvidÃªncias:\n\n1. A maioria das tags Ã© uma versÃ£o \"v*\" (ex: v0.7.8) e uma outra Ã© \"docker-rebuild-v*\" (ex: docker-rebuild-v0.7.8). Isso sugere que a estratÃ©gia de releases Ã© uma mistura de LTS (Long-Term Support) e Current.\n2. A mÃ©dia de intervalos entre as tags Ã© de aproximadamente 19 dias, o que Ã© um intervalo razoÃ¡vel para uma estratÃ©gia de releases.\n3. A mÃ©dia de dias entre as tags \"v*\" Ã© de aproximadamente 19 dias, o que Ã© um intervalo razoÃ¡vel para uma estratÃ©gia de releases.\n4. A mÃ©dia de dias entre as tags \"docker-rebuild-*\" Ã© de aproximadamente 17 dias, o que Ã© um intervalo razoÃ¡vel para uma estratÃ©gia de releases.\n5. A mÃ©dia de dias entre as tags \"weird/mistas\" Ã© de aproximadamente 56 dias, o que Ã© um intervalo muito grande e incomum para uma estratÃ©gia de releases. Isso sugere que a estratÃ©gia de releases Ã© uma mistura de LTS e Current.\n6. A maioria das tags Ã© uma versÃ£o \"v*\" ou uma outra Ã© \"docker-rebuild-v*\", o que sugere que a estratÃ©gia de releases Ã© uma mistura de LTS e Current.\n7. A maioria das tags Ã© uma versÃ£o \"v*\" ou uma outra Ã© \"docker-rebuild-v*\", o que sugere que a estratÃ©gia de releases Ã© uma mistura de LTS e Current.\n8. A maioria das tags Ã© uma versÃ£o \"v*\" ou uma outra Ã© \"docker-rebuild-v*\", o que sugere que a estratÃ©gia de releases Ã© uma mistura de LTS e Current.\n9. A maioria das tags Ã© uma versÃ£o \"v*\" ou uma outra Ã© \"docker-rebuild-v*\", o que sugere que a estratÃ©gia de releases Ã© uma mistura de LTS e Current.\n10. A maioria das tags Ã© uma versÃ£o \"v*\" ou uma outra Ã© \"docker-rebuild-v*\", o que sugere que a estratÃ©gia de releases Ã© uma mistura de LTS e Current.\n\nConclusÃ£o: A estratÃ©gia de releases Ã© uma mistura de LTS e Current, com uma mistura de \"v*\" e \"docker-rebuild-v*\". Isso Ã© uma estratÃ©gia razoÃ¡vel para uma aplicaÃ§Ã£o que precisa de uma versÃ£o estÃ¡vel e uma versÃ£o atualizada."},"metadata":{}}],"execution_count":9}]}