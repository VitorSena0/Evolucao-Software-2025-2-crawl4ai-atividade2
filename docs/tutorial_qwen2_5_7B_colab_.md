# ğŸ“˜ Tutorial Completo â€” AnÃ¡lise de Branches e EstratÃ©gia de Releases com LLM  
## Modelo: Qwen/Qwen2.5-7B-Instruct (Google Colab Free)

Este documento descreve **detalhadamente cada cÃ©lula do notebook**, explicando **o que ela faz, por que existe e como contribui para a anÃ¡lise**.  
O objetivo Ã© permitir que **qualquer pessoa replique o experimento** e compreenda as decisÃµes tÃ©cnicas adotadas.

---

## ğŸ¯ Objetivo do Tutorial

Utilizar o modelo de linguagem **Qwen/Qwen2.5-7B-Instruct** para apoiar a identificaÃ§Ã£o de:

- **Modelo de Fluxo de Trabalho (Branching Model)**
- **EstratÃ©gia de Releases (ritmo de entrega)**

A anÃ¡lise Ã© baseada **exclusivamente em dados reais extraÃ­dos de um repositÃ³rio Git**, garantindo rigor metodolÃ³gico.

---

## ğŸ§  Modelo de Linguagem Utilizado

- **Nome:** Qwen/Qwen2.5-7B-Instruct  
- **Plataforma:** Hugging Face  
- **ParÃ¢metros:** 7B  
- **ExecuÃ§Ã£o:** Google Colab Free (GPU T4)  
- **OtimizaÃ§Ã£o:** QuantizaÃ§Ã£o 4-bit com bitsandbytes  

---

## âš ï¸ PrÃ©-requisitos

- Conta Google
- Google Colab com **GPU ativada**
- Navegador web moderno
- (Opcional) Token do Hugging Face

---

## â–¶ï¸ ConfiguraÃ§Ã£o Inicial no Google Colab

1. Acesse: https://colab.research.google.com  
2. Crie um novo notebook  
3. Ative a GPU em:

```
Ambiente de execuÃ§Ã£o â†’ Alterar tipo de ambiente de execuÃ§Ã£o â†’ GPU
```

4. Reinicie o ambiente

---

# ğŸ““ CÃ‰LULAS DO NOTEBOOK â€” COM EXPLICAÃ‡ÃƒO

---

## ğŸ”´ CÃ‰LULA 0 â€” VerificaÃ§Ã£o de GPU (ObrigatÃ³ria)

### ğŸ“Œ Objetivo
Garantir que o notebook **nÃ£o seja executado em CPU**, evitando erros posteriores.

### ğŸ§  Por que isso Ã© necessÃ¡rio?
O modelo Qwen 7B **nÃ£o funciona no Colab Free sem GPU**.  
Essa cÃ©lula interrompe a execuÃ§Ã£o caso a GPU nÃ£o esteja ativa.

```python
import torch

print("Torch:", torch.__version__)
print("CUDA disponÃ­vel:", torch.cuda.is_available())

if not torch.cuda.is_available():
    raise RuntimeError(
        "GPU NÃƒO ATIVA. Ative em Ambiente de execuÃ§Ã£o â†’ GPU e reinicie."
    )

print("GPU detectada:", torch.cuda.get_device_name(0))
```

---

## ğŸ” CÃ‰LULA 1 â€” AutenticaÃ§Ã£o no Hugging Face

### ğŸ“Œ Objetivo
Autenticar o usuÃ¡rio no Hugging Face **caso um token esteja disponÃ­vel**.

### ğŸ§  Por que usar token?
- Evita rate limit
- Aumenta a estabilidade do download
- NÃ£o Ã© obrigatÃ³rio (modelo pÃºblico)

```python
import os
from huggingface_hub import login

if "HUGGINGFACE_TOKEN" in os.environ:
    login(token=os.environ["HUGGINGFACE_TOKEN"])
    print("Hugging Face autenticado com token")
else:
    print("Token nÃ£o encontrado â€” acesso pÃºblico serÃ¡ utilizado")
```

---

## âš™ï¸ CÃ‰LULA 2 â€” Setup do Ambiente

### ğŸ“Œ Objetivo
Instalar apenas as dependÃªncias necessÃ¡rias, **sem reinstalar o PyTorch**, evitando conflitos com CUDA.

### ğŸ§  DecisÃµes importantes
- O PyTorch do Colab jÃ¡ vem com CUDA
- bitsandbytes Ã© reinstalado para garantir compatibilidade

```python
import os, sys, subprocess

os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

def pip_install(args):
    subprocess.check_call([sys.executable, "-m", "pip"] + args.split())

def pip_uninstall(pkg):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "uninstall", "-y", pkg])
    except:
        pass

pip_install("install -q -U transformers accelerate huggingface_hub")
pip_uninstall("bitsandbytes")
pip_install("install -q -U bitsandbytes")

print("Ambiente configurado com sucesso")
```

---

## ğŸ§  CÃ‰LULA 3 â€” Carregamento do Modelo (Qwen 7B)

### ğŸ“Œ Objetivo
Carregar o modelo **Qwen2.5-7B-Instruct** de forma segura no Colab Free.

### ğŸ§  EstratÃ©gias usadas
- QuantizaÃ§Ã£o 4-bit
- ForÃ§ar uso exclusivo da GPU
- Evitar offload automÃ¡tico para RAM

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

MODEL_ID = "Qwen/Qwen2.5-7B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_ID,
    use_fast=True,
    trust_remote_code=True
)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map={"": 0},
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    trust_remote_code=True,
)

model.eval()

print("Modelo Qwen2.5-7B carregado com sucesso")
```

---

## ğŸ§© CÃ‰LULA 4 â€” FunÃ§Ã£o de InferÃªncia Controlada

### ğŸ“Œ Objetivo
Criar uma funÃ§Ã£o que:
- Limite o nÃºmero de tokens
- Evite estouro de memÃ³ria
- Garanta respostas consistentes

```python
def ask_qwen(
    prompt,
    max_new_tokens=700,
    temperature=0.2,
    top_p=0.95,
    max_prompt_tokens=6000,
):
    messages = [
        {"role": "system", "content": "VocÃª Ã© um especialista em Engenharia de Software."},
        {"role": "user", "content": prompt},
    ]

    input_ids = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        truncation=True,
        max_length=max_prompt_tokens,
        return_tensors="pt"
    ).to(model.device)

    with torch.inference_mode():
        output = model.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            pad_token_id=tokenizer.eos_token_id,
        )

    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    return decoded.split(messages[-1]["content"])[-1].strip()
```

---

## ğŸ§ª CÃ‰LULA 5 â€” Teste do Modelo

### ğŸ“Œ Objetivo
Validar se o modelo estÃ¡ respondendo corretamente antes de prosseguir.

```python
print(ask_qwen("Explique em duas linhas o que Ã© GitFlow."))
```

---

## ğŸ“¦ CÃ‰LULA 6 â€” Coleta de EvidÃªncias do Git

### ğŸ“Œ Objetivo
Extrair dados reais do repositÃ³rio:
- branches
- tags
- distribuiÃ§Ã£o por prefixo

Esses dados sÃ£o a **base factual** da anÃ¡lise.

```python
import subprocess, os
from collections import Counter

def git(cmd, cwd):
    return subprocess.check_output(cmd, cwd=cwd, shell=True).decode().strip()

repo_url = "https://github.com/unclecode/crawl4ai.git"
repo_dir = "/content/repo"

if not os.path.exists(repo_dir):
    subprocess.check_call(["git", "clone", repo_url, repo_dir])

branches = git("git branch -r | grep -v HEAD", repo_dir).splitlines()
branches = [b.replace("origin/", "").strip() for b in branches]

tags = git("git tag --list", repo_dir).splitlines()

bucket = Counter(b.split('/')[0] if '/' in b else 'root' for b in branches)

print("Branches:", len(branches))
print("Tags:", len(tags))
print("DistribuiÃ§Ã£o:", bucket)
```

---

## ğŸŒ¿ CÃ‰LULA 7 â€” AnÃ¡lise do Branching Model

### ğŸ“Œ Objetivo
Solicitar ao modelo que identifique o **modelo de fluxo de trabalho**, utilizando apenas as evidÃªncias coletadas.

```python
prompt = f"""
Analise o modelo de branching com base nos dados reais.

Branches totais: {len(branches)}
DistribuiÃ§Ã£o por prefixo:
{bucket}

Classifique:
- GitFlow
- GitHub Flow
- Trunk-Based Development

Justifique com base apenas nas evidÃªncias.
"""

print(ask_qwen(prompt))
```

---

## â±ï¸ CÃ‰LULA 8 â€” AnÃ¡lise da EstratÃ©gia de Releases

### ğŸ“Œ Objetivo
Identificar o ritmo de entrega do projeto a partir das tags.

```python
prompt = f"""
Analise a estratÃ©gia de releases com base nas tags abaixo.

Total de tags: {len(tags)}
Amostra de tags:
{tags[-20:]}

Classifique:
- Rapid Release
- Release Train
- LTS + Current
- Ad-hoc
"""

print(ask_qwen(prompt))
```

---

## âœ… ConsideraÃ§Ãµes Finais

Este tutorial evidencia que **LLMs podem apoiar anÃ¡lises de governanÃ§a de software**, desde que:

- sejam alimentados com **dados reais**
- utilizem **prompts restritivos**
- respeitem **limitaÃ§Ãµes do ambiente de execuÃ§Ã£o**

O modelo **Qwen2.5-7B-Instruct** demonstrou bom desempenho na identificaÃ§Ã£o de padrÃµes estruturais em projetos open source.

